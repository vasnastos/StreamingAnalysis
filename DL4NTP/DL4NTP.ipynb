{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DL4NTP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/fle1scha/DL4NTP/blob/main/DL4NTP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add all imports and print TensorFlow version to verify it is correct version"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "from datetime import datetime, date\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers  import LSTM\n",
        "from tensorflow.keras.layers  import TimeDistributed\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from numpy import array\n",
        "from numpy import cumsum\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from random import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from matplotlib import pyplot\n",
        "\n",
        "print(keras.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data"
      ],
      "metadata": {
        "id": "O545X4ogU63c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mount Google Drive file system"
      ],
      "metadata": {
        "id": "apW7Ec7Q6cZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "source": [
        "# from google.colab import drive # Hosting the file in Google Drive, need to mount the drive so it is accessible\n",
        "# # Currently Google forces an authorisation code, local runtime would rectify this\n",
        "# drive.mount('/content/gdrive', force_remount=True) #force_remount forces Google t"
      ],
      "outputs": [],
      "metadata": {
        "id": "6CHuCizHipju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4bc5d0-243f-48e3-c098-8ced3f965ee1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "source": [
        "#Read in SANREN dataset.\n",
        "with open('SANREN_large.txt') as f:\n",
        "  SANReN = f.readlines()"
      ],
      "outputs": [],
      "metadata": {
        "id": "bMMf07H7mA3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37963be7-c76c-4f9e-bc89-f12777bd2ba7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Preprocessing***"
      ],
      "metadata": {
        "id": "3QMDbVDehWb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "source": [
        "#Clean dataframe headers and create labels.\n",
        "headings_line = SANReN[0].split()\n",
        "\n",
        "headings_line[4:7] = [''.join(headings_line[4:7])] #Merge 'Src', 'IP', and 'Addr:Port' \n",
        "headings_line[5:8] = [''.join(headings_line[5:8])] #Merge 'Dst', 'IP', and 'Addr:Port' \n",
        "headings_line = headings_line[0:6] + headings_line[8:13] #Remove 'Flags', 'Tos', and 'Flows'.\n",
        "\n",
        "print(headings_line)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'first-seen', 'Duration', 'Proto', 'SrcIPAddr:Port', 'DstIPAddr:Port', 'Packets', 'Bytes', 'pps', 'bps', 'Bpp']\n"
          ]
        }
      ],
      "metadata": {
        "id": "Wn5UvaMAtfxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "source": [
        "#Clean time-series data points. \n",
        "framedata = []\n",
        "for i in range(1, 20001):\n",
        "  data_line = SANReN[i].split()\n",
        "  #print(headings_line)\n",
        "  #sprint(data_line)\n",
        "  #print(data_line)\n",
        "  measures = ['M', 'G']\n",
        "  if ((data_line[11] == \"M\" or data_line[11] == 'G') and (data_line[13] == 'M' or data_line[13] == 'G') and (data_line[15] == 'M' or data_line[15] == 'G')):\n",
        "    if (data_line[11] == 'G'):\n",
        "      data_line[10] = float(data_line[10])*100000000\n",
        "    else:\n",
        "      data_line[10] = float(data_line[10])*1000000\\\n",
        "\n",
        "    if (data_line[13] == 'G'):\n",
        "      data_line[12] = float(data_line[12])*100000000\n",
        "    else:\n",
        "      data_line[12] = float(data_line[12])*1000000\n",
        "\n",
        "    if (data_line[15] == 'G'):\n",
        "      data_line[14] = float(data_line[14])*100000000\n",
        "    else:\n",
        "      data_line[14] = float(data_line[14])*1000000\n",
        "    \n",
        "\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:13] + data_line[14:15] + data_line[16:17]\n",
        "    #print(data_line)\n",
        "\n",
        "  elif ((data_line[11] == \"M\" or data_line[11] == 'G') and (data_line[14] == 'M' or data_line[14] == 'G')): #Bytes and BPS in megabytes\\n\"\n",
        "    #print(\"1 and 2\") \n",
        "    if (data_line[11] == 'G'):\n",
        "      data_line[10] = float(data_line[10])*100000000\n",
        "    else:\n",
        "      data_line[10] = float(data_line[10])*1000000\\\n",
        "\n",
        "    if (data_line[14] == 'G'):\n",
        "      data_line[13] = float(data_line[13])*100000000\n",
        "    else:\n",
        "      data_line[13] = float(data_line[13])*1000000\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:14] + data_line[15:16]\n",
        "  \n",
        "  elif ((data_line[12] == \"M\" or data_line[12] == 'G') and (data_line[12] == 'M' or data_line[12] == 'G')): #Bytes and BPS in megabytes\\n\"\n",
        "    #print(\"1 and 2\") \n",
        "    if (data_line[12] == 'G'):\n",
        "      data_line[11] = float(data_line[11])*100000000\n",
        "    else:\n",
        "      data_line[11] = float(data_line[11])*1000000\\\n",
        "\n",
        "    if (data_line[14] == 'G'):\n",
        "      data_line[13] = float(data_line[13])*100000000\n",
        "    else:\n",
        "      data_line[13] = float(data_line[13])*1000000\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:12] + data_line[13:14] + data_line[15:16]\n",
        "     \n",
        "  elif (data_line[13] == 'M' or data_line[13] == 'G'): #BPS measured in megabytes\n",
        "    #print(\"2\")\n",
        "    if (data_line[13] == 'G'):\n",
        "      data_line[12] = float(data_line[12])*100000000\n",
        "    \n",
        "    else:\n",
        "      data_line[12] = float(data_line[12])*1000000\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:13] + data_line[14:15]\n",
        "  \n",
        "  elif data_line[11] == 'M': #Bytes measured in megabytes\n",
        "    #print(\"1\")\n",
        "    \n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:11] + data_line[12:15]\n",
        "    data_line[7] = float(data_line[7])*1000000 #Change M bytes into byte measurement. \n",
        "    \n",
        "  else: #No megabyte metrics\n",
        "    #print(\"0\")\n",
        "    data_line = data_line[0:5] + data_line[6:7] + data_line[9:14]\n",
        "   \n",
        "\n",
        "  #data_line  = np.asarray(data_line) #Turn each line into a NumPy array.\n",
        "  framedata.append(data_line) #append each line to 'mother' array."
      ],
      "outputs": [],
      "metadata": {
        "id": "W6uYg9vVuEcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "source": [
        "#Convert Numpy array into Pandas dataframe.\n",
        "df = pd.DataFrame(np.array(framedata), columns=headings_line) \n",
        "print(df.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 11)\n"
          ]
        }
      ],
      "metadata": {
        "id": "yS4XVsJv4cbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "source": [
        "#Define all data types correctly. \n",
        "df['Datetimetemp'] = df['Date'] + ' ' + df['first-seen'] #Combine Date and first-seen\n",
        "df = df.astype({'Date': 'datetime64[ns]'})\n",
        "df[\"Day\"] = df['Date'].dt.dayofweek #Created Day variable.\n",
        "df = df.astype({'Date': str})\n",
        "#df = df.astype({'first-seen': np.datetime64})\n",
        "df = df.astype({'Duration': np.float64})\n",
        "df = df.astype({\"SrcIPAddr:Port\": str})\n",
        "df = df.astype({\"DstIPAddr:Port\": str})\n",
        "df = df.astype({\"Packets\": np.int64})\n",
        "df = df.astype({\"Bytes\": np.float64})\n",
        "df = df.astype({\"pps\": np.float64})\n",
        "df = df.astype({\"bps\": np.float64})\n",
        "df = df.astype({\"Bpp\": np.float64})\n",
        "\n",
        "#Create binary Weekend variable.\n",
        "df['Weekend'] = 0\n",
        "df.loc[df['Day'] == 5 , 'Weekend'] = 1\n",
        "df.loc[df['Day'] == 6 , 'Weekend'] = 1\n",
        "\n",
        "#Insert combined Datetime at front of dataframe.\n",
        "df.insert(0, 'Datetime', df['Datetimetemp'])\n",
        "df['Datetime'] = df.Datetime.astype('datetime64[ns]')\n",
        "df['Datetime'] = df.Datetime.astype('int64') #Convert Datetime into an integer representation. This is a deprecated method. \n",
        "\n",
        "#Define university holiday calender\n",
        "holidays = pd.date_range(start='2020-1-1', end='2020-3-14', freq = '1D')\n",
        "holidays = holidays.append(pd.date_range(start='2020-5-1', end='2020-5-9', freq='1D'))\n",
        "holidays = holidays.append(pd.date_range(start='2020-07-04', end='2020-08-02', freq='1D'))\n",
        "holidays = holidays.append(pd.date_range(start='2020-09-18', end='2020-09-27', freq='1D'))\n",
        "holidays = holidays.append(pd.date_range(start='2020-11-24', end='2020-12-31', freq='1D'))\n",
        "holidays = holidays.strftime(\"%Y-%m-%d\").tolist()\n",
        "#sprint(df['Date'])\n",
        "#Add Holiday column to dataframe.\n",
        "df['Holiday'] = 0\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/folders/r0/zt6nwh1x7_lb4ddlk_6krd2c0000gn/T/ipykernel_40732/1860391016.py:24: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
            "  df['Datetime'] = df.Datetime.astype('int64') #Convert Datetime into an integer representation. This is a deprecated method.\n"
          ]
        }
      ],
      "metadata": {
        "id": "3ePYkBWYgO1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "source": [
        "for date in holidays:\n",
        "    df.loc[df['Date'] == date, 'Holiday'] = 1"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "source": [
        "print(df['Date'].values[1])\n",
        "print(holidays[83])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-07-04\n",
            "2020-07-04\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "source": [
        "print(df['Holiday'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        1\n",
            "4        1\n",
            "        ..\n",
            "19995    1\n",
            "19996    1\n",
            "19997    1\n",
            "19998    1\n",
            "19999    1\n",
            "Name: Holiday, Length: 20000, dtype: int64\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Delete unused columns.\n",
        "del df['Date']\n",
        "del df['first-seen']\n",
        "del df['Datetimetemp']"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#One-Hot Encoding\n",
        "category_df = (df.copy().drop(['Day', 'Weekend', 'Holiday', 'SrcIPAddr:Port', 'DstIPAddr:Port', 'Datetime', 'Duration', 'Packets', 'Bytes', 'pps', 'bps', 'Bpp'], axis = 1))\n",
        "for x in category_df.columns:\n",
        "    #Printing unique values per categorical variable\n",
        "    print(x ,':', len(category_df[x].unique()))\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "#Transform data\n",
        "onehot = encoder.fit_transform(category_df)\n",
        "category_df = pd.DataFrame(np.array(onehot))\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Explore individual categories\n",
        "print(df.head(1))\n",
        "\n",
        "groups = [1, 5, 6, 7, 8, 9]\n",
        "values = df.values\n",
        "i = 1\n",
        "# plot each column\n",
        "pyplot.figure()\n",
        "for group in groups:\n",
        "\tpyplot.subplot(len(groups), 1, i)\n",
        "\n",
        "\tpyplot.plot(values[:, group])\n",
        "\tpyplot.title(df.columns[group], y=0.5, loc='right')\n",
        "\ti += 1\n",
        "pyplot.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#A plot of Bytes vs Datetime\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Bytes vs Datetime\")\n",
        "plt.scatter(df['Datetime'], df['Bytes']) #changed to scatter because line graph is very bunched. fromordinal only differentiates by date so will have to find a way to get it into seconds. \n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Add encoded categorical data to regression data\n",
        "df = (df.copy().drop(['SrcIPAddr:Port', 'DstIPAddr:Port', 'Proto'], axis = 1)).copy()\n",
        "print(df.shape)\n",
        "df = pd.concat([df, category_df], axis = 1)\n",
        "print(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JrT_zzX-vkNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Split data into both training and test set. Use 80/20 split.\n",
        "test_size = int(len(df) * 0.2) # the test data will be 20% (0.2) of the sample.\n",
        "train = df.iloc[:-test_size,:].copy()  #Not copying here threw an error. Must be careful not to keep two copies for memory reasons.\n",
        "test = df.iloc[-test_size:,:].copy() \n",
        "\n",
        "X_train = train.drop('Bytes',axis=1).copy() #Drop target variable from training data. \n",
        "y_train = train[['Bytes']].copy() # The double brackets are to keep Bytes in a pandas dataframe format, otherwise it will be pandas Series.\n",
        "print(X_train.shape, y_train.shape) #Check shape of training variables. "
      ],
      "outputs": [],
      "metadata": {
        "id": "JWcYnu049hVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d034df08-e31d-46a9-9042-70a1191cdd94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Visualise split in sample\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.title(\"Split of Test and Train Set using Bytes as Target Variable\")\n",
        "plt.scatter(train['Datetime'],train['Bytes'],label='Training set')\n",
        "plt.scatter(test['Datetime'],test['Bytes'],label='Test set')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "metadata": {
        "id": "q4HrUcbr3R6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "746b7ba2-aab1-4c60-df13-d33bfadc9a1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Scale training dating\n",
        "Xscaler = MinMaxScaler(feature_range=(0, 1)) # scikit MinMixScaler allows all variables to be normalised between 0 and 1.\n",
        "Xscaler.fit(X_train) #Compute the minimum and maximum to be used for later scaling\n",
        "scaled_X_train = Xscaler.transform(X_train) #Scale features of X according to feature_range.\n",
        "\n",
        "print(X_train.shape) #X_train shape is the same as earlier but now scaled. \n",
        "print(scaled_X_train) #Demonstrate normalised data. "
      ],
      "outputs": [],
      "metadata": {
        "id": "8BGIUeb_3JCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Scale training response\n",
        "Yscaler = MinMaxScaler(feature_range=(0, 1)) #apply same normalisation to response. \n",
        "Yscaler.fit(y_train)\n",
        "scaled_y_train = Yscaler.transform(y_train)\n",
        "scaled_y_train = np.insert(scaled_y_train, 0, 0)\n",
        "scaled_y_train = np.delete(scaled_y_train, -1)\n",
        "\n",
        "print(scaled_y_train.shape) #Shape is constant. "
      ],
      "outputs": [],
      "metadata": {
        "id": "XMobcpqrpmAY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Define input_shape for LSTM\n",
        "n_input = 100 #how many samples/rows/timesteps to look in the past in order to forecast the next sample\n",
        "n_features= X_train.shape[1] # how many predictors/Xs/features we have to predict y\n",
        "b_size = 100 # Number of timeseries samples in each batch\n",
        "train_generator = TimeseriesGenerator(scaled_X_train, scaled_y_train, length=n_input, batch_size=b_size)\n",
        "print(train_generator[0][0].shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rcdTgS6S3L8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple LSTM Implementation"
      ],
      "metadata": {
        "id": "iqmDXFuAhFzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# The sequential argument means that we can add layers without worrying about the underlying shape of the tensors\n",
        "simple_LSTM_model = Sequential() \n",
        "# Model is an LSTM, 50 is dimentionality of the output, activation function is relu\n",
        "simple_LSTM_model.add(LSTM(50, activation='sigmoid', input_shape=(n_input, n_features))) \n",
        "# Dense layer as the first layer of the model\n",
        "simple_LSTM_model.add(Dense(1)) \n",
        "# Compile the model with the adam optimizer, loss measured in Mean Squarred Error\n",
        "# Adam refers to the learning rate change, which is measured by the exponentially decaying average of past gradients\n",
        "simple_LSTM_model.compile(optimizer='adam', loss='mse') \n",
        "# Print out a summary of the LSTM to check that it was compiled correctly \n",
        "simple_LSTM_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ukpmoRaA3OZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Fit the data to the model and train.\n",
        "simple_LSTM_model.fit(train_generator, epochs=5, verbose = 1) # Fit the features excluding target, and predict the target value\n",
        "# verbose of 0 hides the training, 2 shows the full log\n",
        "loss_per_epoch = simple_lstm_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "O3hK7I40Hr-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Testing x data\n",
        "X_test = test.drop('Bytes', axis=1).copy()\n",
        "scaled_X_test = Xscaler.transform(X_test)\n",
        "test_generator = TimeseriesGenerator(scaled_X_test, np.zeros(len(X_test)), length=100, batch_size=b_size) #There are only 17 samples in the test set so it cannot look back.\n",
        "print(test_generator[0][0].shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "V9Sy1TcBMBLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9126c4-2691-40b9-8954-852c21024c38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Predicted LSTM response for bytes\n",
        "y_pred_scaled = simple_LSTM_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "simple_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:],'y_pred':y_pred.ravel()})\n",
        "simple_lstm_results.plot()\n",
        "print(simple_lstm_results)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ikSwTb0VjTAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Predicted  LSTM response in megabytes\n",
        "simple_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "simple_lstm_results['residuals'] = np.square(\n",
        "    simple_lstm_results.y_pred - simple_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "simple_LSTM_mse = simple_lstm_results.residuals.sum() * (1/len(simple_lstm_results))\n",
        "print('RMSE: ' + str(np.round(rmse(simple_lstm_results.y_true, simple_lstm_results.y_pred), 3)))\n",
        "print('MSE: ' + str(np.round(simple_LSTM_mse, 3)))\n",
        "print('MAE: ' + str(mean_absolute_error(simple_lstm_results.y_true, simple_lstm_results.y_pred)))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6dSsN3FlbX0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# LSTM Coefficient of Determination Measurement\n",
        "simple_LSTM_correlation_matrix = np.corrcoef(simple_lstm_results.y_pred,\n",
        "                          simple_lstm_results.y_true)\n",
        "corr = simple_LSTM_correlation_matrix[0, 1]\n",
        "simple_LSTM_R_sq = corr**2\n",
        "print(np.round(simple_LSTM_R_sq, 5))\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional LSTM Implementation"
      ],
      "metadata": {
        "id": "zHd51XCpZTaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Bidirectional LSTM supported in Keras using a layer wrapper \n",
        "# Common approach is to use concatenate, providing 2x outputs to next layer\n",
        "# Takes the first LSTM layer as an argument\n",
        "# The sequential argument means that we can add layers without worrying about the underlying shape of the tensors\n",
        "bidirectional_lstm_model = Sequential() \n",
        "bidirectional_lstm_model.add(Bidirectional(LSTM(50, return_sequences=False, activation=\"sigmoid\"), input_shape=(n_input, n_features)))\n",
        "bidirectional_lstm_model.add(Dense(1))\n",
        "bidirectional_lstm_model.compile(loss=rmse, optimizer='adam')\n",
        "bidirectional_lstm_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "lKV5YX2hZTG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bidirectional_lstm_model.fit(train_generator, epochs=5, verbose=2)\n",
        "loss_per_epoch = bidirectional_lstm_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KOBwSV7ZCjSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred_scaled = bidirectional_lstm_model.predict(test_generator)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "bidirectional_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:],'y_pred':y_pred.ravel()})\n",
        "print(bidirectional_lstm_results)\n",
        "bidirectional_lstm_results.plot()"
      ],
      "outputs": [],
      "metadata": {
        "id": "58GdL-YpErOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions made in megabytes"
      ],
      "metadata": {
        "id": "hoUVo7Yhb-W-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "bidirectional_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:]/1000000,'y_pred':y_pred.ravel()/1000000})\n",
        "bidirectional_lstm_results['residuals'] = np.square(bidirectional_lstm_results.y_pred - bidirectional_lstm_results.y_true)\n",
        "# results['residuals_squared'] = np.square(results.residuals)\n",
        "bidirectional_LSTM_mse = bidirectional_lstm_results.residuals.sum() * (1/len(bidirectional_lstm_results))\n",
        "print('RMSE: ' + str(np.round(rmse(bidirectional_lstm_results.y_true,\n",
        "      bidirectional_lstm_results.y_pred), 3)))\n",
        "print('MSE: ' + str(np.round(bidirectional_LSTM_mse, 3)))\n",
        "print('MAE: ' + str(mean_absolute_error(bidirectional_lstm_results.y_true,\n",
        "      bidirectional_lstm_results.y_pred)))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "4_X_wcZocA62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# LSTM Coefficient of Determination Measurement\n",
        "bidirectional_LSTM_correlation_matrix = np.corrcoef(bidirectional_lstm_results.y_pred,\n",
        "                                                    bidirectional_lstm_results.y_true)\n",
        "corr = bidirectional_LSTM_correlation_matrix[0, 1]\n",
        "bidirectional_LSTM_R_sq = corr**2\n",
        "print(np.round(bidirectional_LSTM_R_sq, 5))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacked LSTM Implementation"
      ],
      "metadata": {
        "id": "k7L7w7oFCbfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stacked_lstm_model = Sequential() # The sequential argument means that we can add layers without worrying about the underlying shape of the tensors \n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True, activation=\"sigmoid\", input_shape=(n_input, n_features)))\n",
        "stacked_lstm_model.add(LSTM(50, return_sequences=True))\n",
        "stacked_lstm_model.add(LSTM(50))\n",
        "stacked_lstm_model.add(Dense(1))\n",
        "stacked_lstm_model.compile(loss=rmse, optimizer='adam')\n",
        "stacked_lstm_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "mIqJ3Qj6Cg49"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stacked_lstm_model.fit(train_generator, epochs=5, verbose=1)\n",
        "loss_per_epoch = stacked_lstm_model.history.history['loss']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BcNDyvGbDUPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred_scaled = stacked_lstm_model.predict(test_generator)\n",
        "print(y_pred_scaled)\n",
        "y_pred = Yscaler.inverse_transform(y_pred_scaled)\n",
        "stacked_lstm_results = pd.DataFrame({'y_true':test['Bytes'].values[100:], 'y_pred':y_pred.ravel()})\n",
        "print(stacked_lstm_results)\n",
        "stacked_lstm_results.plot()\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "cZCO8p0iExFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions made in megabytes"
      ],
      "metadata": {
        "id": "yT2KUuzFcX-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stacked_lstm_results = pd.DataFrame(\n",
        "    {'y_true': test['Bytes'].values[100:]/1000000, 'y_pred': y_pred.ravel()/1000000})\n",
        "stacked_lstm_results['residuals'] = np.square(\n",
        "    stacked_lstm_results.y_pred - stacked_lstm_results.y_true)\n",
        "stacked_LSTM_mse = stacked_lstm_results.residuals.sum() * \\\n",
        "    (1/len(stacked_lstm_results))\n",
        "print('RMSE: ' + str(np.round(rmse(stacked_lstm_results.y_true,\n",
        "      stacked_lstm_results.y_pred), 3)))\n",
        "print('MSE: ' + str(np.round(stacked_LSTM_mse, 3)))\n",
        "print('MAE: ' + str(mean_absolute_error(stacked_lstm_results.y_true,\n",
        "      stacked_lstm_results.y_pred)))\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# LSTM Coefficient of Determination Measurement\n",
        "simple_LSTM_correlation_matrix = np.corrcoef(simple_lstm_results.y_pred,\n",
        "                          simple_lstm_results.y_true)\n",
        "corr = simple_LSTM_correlation_matrix[0, 1]\n",
        "simple_LSTM_R_sq = corr**2\n",
        "print(np.round(simple_LSTM_R_sq, 5))\n",
        "\n",
        "# LSTM Coefficient of Determination Measurement\n",
        "bidirectional_LSTM_correlation_matrix = np.corrcoef(bidirectional_lstm_results.y_pred,\n",
        "                                                    bidirectional_lstm_results.y_true)\n",
        "corr = bidirectional_LSTM_correlation_matrix[0, 1]\n",
        "bidirectional_LSTM_R_sq = corr**2\n",
        "print(np.round(bidirectional_LSTM_R_sq, 5))\n",
        "\n",
        "# LSTM Coefficient of Determination Measurement\n",
        "stacked_LSTM_correlation_matrix = np.corrcoef(stacked_lstm_results.y_pred,\n",
        "                                                    stacked_lstm_results.y_true)\n",
        "corr = stacked_LSTM_correlation_matrix[0, 1]\n",
        "stacked_LSTM_R_sq = corr**2\n",
        "print(np.round(stacked_LSTM_R_sq, 5))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('Mean Squared Errors on Test Sets in MB')\n",
        "print('Bidirectional LSTM MSE: ' + str(np.round(bidirectional_LSTM_mse, 3)))\n",
        "print('Simple LSTM MSE: ' + str(np.round(simple_LSTM_mse, 3)))\n",
        "print('Stacked LSTM MSE: ' + str(np.round(stacked_LSTM_mse, 3)))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coefficient of Determination = R^2\n",
        "- Regression error metric, evaluate the accuracy and efficiency of a model\n",
        "- Describes the variation in response or target variable which is predicted by independent variables of the data model"
      ],
      "metadata": {}
    }
  ]
}